{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from itertools import product\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 2\n",
    "num_actions = 2\n",
    "states = [0, 1]\n",
    "actions = [0, 1]\n",
    "\n",
    "\n",
    "def mdp(state: int, action: int) -> tuple[int, float]:\n",
    "    assert state in [0, 1], \"Invalid state\"\n",
    "    assert action in [0, 1], \"Invalid action\"\n",
    "\n",
    "    if state == 0:\n",
    "        if action == 0:\n",
    "            next_state = 0 if np.random.rand() < 0.9 else 1\n",
    "            reward = np.random.normal(2.0, 1.0)\n",
    "            return next_state, reward\n",
    "        elif action == 1:\n",
    "            next_state = 0 if np.random.rand() < 0.1 else 1\n",
    "            reward = np.random.normal(1.5, 1.0)\n",
    "            return next_state, reward\n",
    "    elif state == 1:\n",
    "        if action == 0:\n",
    "            next_state = 0 if np.random.rand() < 0.9 else 1\n",
    "            reward = np.random.normal(0.0, 1.0)\n",
    "            return next_state, reward\n",
    "        elif action == 1:\n",
    "            next_state = 0 if np.random.rand() < 0.2 else 1\n",
    "            reward = np.random.normal(3.0, 1.0)\n",
    "            return next_state, reward\n",
    "    assert False, \"Should not reach here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 1, Action: 1, Reward: 1.7644137334598737\n",
      "State: 1, Action: 0, Reward: 0.5918058443861409\n",
      "State: 1, Action: 0, Reward: -1.172098637360545\n",
      "State: 0, Action: 1, Reward: 1.9151350957581499\n",
      "State: 1, Action: 0, Reward: -0.04030941450177798\n",
      "State: 0, Action: 1, Reward: 1.081824572708424\n",
      "State: 1, Action: 1, Reward: 3.3165145224564534\n",
      "State: 1, Action: 1, Reward: 0.9866620231437442\n",
      "State: 0, Action: 0, Reward: 1.394226635556238\n",
      "State: 0, Action: 0, Reward: 2.307613019086643\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Sample a trajectory\n",
    "s = np.random.choice(states)\n",
    "a = np.random.choice(actions)\n",
    "state_traj = [s]\n",
    "action_traj = [a]\n",
    "reward_traj = []\n",
    "\n",
    "for _ in range(300):\n",
    "    s_, reward = mdp(s, a)\n",
    "    a_ = np.random.choice(actions)\n",
    "\n",
    "    state_traj.append(s_)\n",
    "    action_traj.append(a_)\n",
    "    reward_traj.append(reward)\n",
    "    s = s_\n",
    "    a = a_\n",
    "\n",
    "for s, a, r in list(zip(state_traj, action_traj, reward_traj))[:10]:\n",
    "    print(f\"State: {s}, Action: {a}, Reward: {r}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_model(states, actions, rewards, next_states):\n",
    "    # Prior for transition probabilities for each state-action pair\n",
    "    alpha_prior = jnp.array([\n",
    "        [[1.0, 1.0], [1.0, 1.0]],\n",
    "        [[1.0, 1.0], [1.0, 1.0]]\n",
    "    ])\n",
    "\n",
    "    trans_probs = numpyro.sample(\n",
    "        \"trans_probs\",\n",
    "        dist.Dirichlet(alpha_prior).expand((num_states, num_actions))\n",
    "    )\n",
    "\n",
    "    # Prior for reward means\n",
    "    reward_means = numpyro.sample(\n",
    "        \"reward_means\",\n",
    "        dist.Normal(0, 5.0).expand((num_states, num_actions))\n",
    "    )\n",
    "\n",
    "    reward_stds = numpyro.sample(\n",
    "        \"reward_stds\",\n",
    "        dist.HalfNormal(5.0).expand((num_states, num_actions))\n",
    "    )\n",
    "\n",
    "    with numpyro.plate(\"record\", len(states)):\n",
    "        numpyro.sample(\n",
    "            \"trans\",\n",
    "            dist.Categorical(probs=trans_probs[states, actions]),\n",
    "            obs=next_states\n",
    "        )\n",
    "\n",
    "        # Gaussian likelihood for rewards\n",
    "        numpyro.sample(\n",
    "            \"reward\",\n",
    "            dist.Normal(\n",
    "                reward_means[states, actions],\n",
    "                reward_stds[states, actions]\n",
    "            ),\n",
    "            obs=rewards\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_traj = jnp.array(state_traj[1:])\n",
    "state_traj = jnp.array(state_traj[:300])\n",
    "action_traj = jnp.array(action_traj[:300])\n",
    "reward_traj = jnp.array(reward_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 3000/3000 [00:03<00:00, 914.88it/s, 7 steps of size 5.69e-01. acc. prob=0.92] \n"
     ]
    }
   ],
   "source": [
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "kernel = NUTS(mdp_model)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\n",
    "mcmc.run(subkey, state_traj, action_traj, reward_traj, next_state_traj)\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities:\n",
      "P(s' | s=0, a=0) = [0.87283117 0.12716895]\n",
      "P(s' | s=0, a=1) = [0.08410239 0.9158976 ]\n",
      "P(s' | s=1, a=0) = [0.804974   0.19502601]\n",
      "P(s' | s=1, a=1) = [0.14632195 0.85367817]\n",
      "Reward means:\n",
      "E[r | s=0, a=0] = 2.0038230419158936\n",
      "E[r | s=0, a=1] = 1.5468661785125732\n",
      "E[r | s=1, a=0] = -0.12734664976596832\n",
      "E[r | s=1, a=1] = 2.970562696456909\n"
     ]
    }
   ],
   "source": [
    "trans_probs_mean = jnp.mean(samples['trans_probs'], axis=0)\n",
    "reward_means_mean = jnp.mean(samples['reward_means'], axis=0)\n",
    "\n",
    "# Print the learned transition probabilities\n",
    "print(\"Transition probabilities:\")\n",
    "for s, a in product(states, actions):\n",
    "    print(f\"P(s' | s={s}, a={a}) = {trans_probs_mean[s, a]}\")\n",
    "\n",
    "# Print the learned reward means\n",
    "print(\"Reward means:\")\n",
    "for s, a in product(states, actions):\n",
    "    print(f\"E[r | s={s}, a={a}] = {reward_means_mean[s, a]}\")\n",
    "\n",
    "# Transition probabilities:\n",
    "# P(s' | s=0, a=0) = [0.87283117 0.12716895]\n",
    "# P(s' | s=0, a=1) = [0.08410239 0.9158976 ]\n",
    "# P(s' | s=1, a=0) = [0.804974   0.19502601]\n",
    "# P(s' | s=1, a=1) = [0.14632195 0.85367817]\n",
    "# Reward means:\n",
    "# E[r | s=0, a=0] = 2.0038230419158936\n",
    "# E[r | s=0, a=1] = 1.5468661785125732\n",
    "# E[r | s=1, a=0] = -0.12734664976596832\n",
    "# E[r | s=1, a=1] = 2.970562696456909\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.87283117, 0.12716895],\n",
       "        [0.08410239, 0.9158976 ]],\n",
       "\n",
       "       [[0.804974  , 0.19502601],\n",
       "        [0.14632195, 0.85367817]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
